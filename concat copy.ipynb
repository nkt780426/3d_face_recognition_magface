{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c6f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T18:44:18.731566Z",
     "iopub.status.busy": "2024-12-23T18:44:18.731295Z",
     "iopub.status.idle": "2024-12-23T18:44:37.075895Z",
     "shell.execute_reply": "2024-12-23T18:44:37.074927Z"
    },
    "papermill": {
     "duration": 18.351133,
     "end_time": "2024-12-23T18:44:37.077621",
     "exception": false,
     "start_time": "2024-12-23T18:44:18.726488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from going_modular.dataloader.multitask import create_multitask_datafetcher\n",
    "from going_modular.model.MTLFaceRecognition import MTLFaceRecognition\n",
    "from going_modular.loss.MultiTaskLoss import MultiTaskLoss\n",
    "from going_modular.train_eval.train import fit\n",
    "from going_modular.utils.transforms import RandomResizedCropRect, GaussianNoise\n",
    "from going_modular.utils.MultiMetricEarlyStopping import MultiMetricEarlyStopping\n",
    "from going_modular.utils.ModelCheckPoint import ModelCheckpoint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Đặt seed toàn cục\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 30,\n",
    "    'loss_da_gender_weight': 30,\n",
    "    'loss_emotion_weight': 5,\n",
    "    'loss_da_emotion_weight': 5,\n",
    "    'loss_pose_weight': 30,\n",
    "    'loss_da_pose_weight': 30,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_da_spectacles_weight': 5,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "    'loss_da_facial_hair_weight': 5,\n",
    "}\n",
    "\n",
    "CONFIGURATION['num_classes'] = len(os.listdir('./Dataset/Albedo/train'))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    RandomResizedCropRect(256),\n",
    "    GaussianNoise(),\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=CONFIGURATION['image_size'], width=CONFIGURATION['image_size'])\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "train_dataloader, test_dataloader, train_weight_class = create_multitask_datafetcher(CONFIGURATION, train_transform, test_transform)\n",
    "model = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "\n",
    "criterion = MultiTaskLoss(os.path.join(CONFIGURATION['dataset_dir'], 'train_set.csv'), CONFIGURATION)\n",
    "optimizer = Adam(model.parameters(), lr=CONFIGURATION['base_lr'])\n",
    "# Khởi tạo scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "earlystop_dir = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models')\n",
    "\n",
    "early_stopping = MultiMetricEarlyStopping(\n",
    "    monitor_keys=['cosine_auc', 'euclidean_auc'],\n",
    "    patience=1000,\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_dir=earlystop_dir,\n",
    "    start_from_epoch=0\n",
    ")      \n",
    "checkpoint_path = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models/checkpoint.pth')\n",
    "modle_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "╒═════════════════════╤════════════╤══════════════════════╕\n",
      "│ Metric              │      Train │ Test                 │\n",
      "╞═════════════════════╪════════════╪══════════════════════╡\n",
      "│ loss                │ 57.1741    │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_id             │ 22.5319    │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_gender         │  0.0591503 │ 0.13695251033641398  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_gender      │  0.65951   │ 0.605582288466394    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_emotion        │  0.185542  │ 0.18040732108056545  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_emotion     │  0.201077  │ 0.2562841195613146   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_pose           │  0.0231053 │ 0.025679882615804672 │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_pose        │  0.0375894 │ 0.08899268787354231  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_facial_hair    │  0.220549  │ 0.23634372372180223  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_facial_hair │  0.398876  │ 0.31528076715767384  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_spectacles     │  0.140655  │ 0.1252289186231792   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_spectacles  │  1.10563   │ 1.2639077678322792   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_gender          │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_spectacles      │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_facial_hair     │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_pose            │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_emotion         │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_cosine       │  0.663679  │ 0.6428744924544345   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_euclidean    │  0.69665   │ 0.5612164042611175   │\n",
      "╘═════════════════════╧════════════╧══════════════════════╛\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 2:\n",
      "╒═════════════════════╤════════════╤══════════════════════╕\n",
      "│ Metric              │      Train │ Test                 │\n",
      "╞═════════════════════╪════════════╪══════════════════════╡\n",
      "│ loss                │ 52.9584    │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_id             │ 23.6073    │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_gender         │  0.0524327 │ 0.06445736368186772  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_gender      │  0.511461  │ 0.1729811665136367   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_emotion        │  0.182828  │ 0.1781366467475891   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_emotion     │  0.245284  │ 0.25718044117093086  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_pose           │  0.0233118 │ 0.021980046993121505 │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_pose        │  0.0442028 │ 0.10666947811841965  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_facial_hair    │  0.212802  │ 0.2446401845663786   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_facial_hair │  0.447379  │ 0.7574046608060598   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_spectacles     │  0.097993  │ 0.14759569615125656  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_spectacles  │  0.895478  │ 0.32647470384836197  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_gender          │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_spectacles      │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_facial_hair     │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_pose            │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_emotion         │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_cosine       │  0.639445  │ 0.6238294623196128   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_euclidean    │  0.659086  │ 0.6305062137806077   │\n",
      "╘═════════════════════╧════════════╧══════════════════════╛\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 3:\n",
      "╒═════════════════════╤════════════╤══════════════════════╕\n",
      "│ Metric              │      Train │ Test                 │\n",
      "╞═════════════════════╪════════════╪══════════════════════╡\n",
      "│ loss                │ 42.4961    │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_id             │ 24.096     │ -                    │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_gender         │  0.046997  │ 0.05769522883929312  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_gender      │  0.157643  │ 0.10372424696106464  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_emotion        │  0.180942  │ 0.1900104582309723   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_emotion     │  0.367456  │ 0.36440522968769073  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_pose           │  0.0226398 │ 0.026995464228093624 │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_pose        │  0.0816692 │ 0.06101887137629092  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_facial_hair    │  0.20754   │ 0.22354458831250668  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_facial_hair │  0.731978  │ 0.5467421393841505   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_spectacles     │  0.0971108 │ 0.08311932231299579  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ loss_da_spectacles  │  0.241294  │ 0.23040872253477573  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_gender          │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_spectacles      │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_facial_hair     │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_pose            │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_emotion         │  1         │ 1.0                  │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_cosine       │  0.691017  │ 0.6670894098638204   │\n",
      "├─────────────────────┼────────────┼──────────────────────┤\n",
      "│ auc_id_euclidean    │  0.710324  │ 0.6813260116474281   │\n",
      "╘═════════════════════╧════════════╧══════════════════════╛\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fit(\n",
    "    conf=CONFIGURATION,\n",
    "    start_epoch=0,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,    \n",
    "    scheduler=scheduler, \n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=modle_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97f2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTLFaceRecognition(\n",
       "  (backbone): MIResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu): PReLU(num_parameters=1)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (spectacles_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (facial_hair_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (emotion_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (pose_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (gender_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (id_head): IdRecognitionModule(\n",
       "    (id_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (maglinear): MagLinear()\n",
       "  )\n",
       "  (gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (grl_gender): GradientReverseLayer()\n",
       "  (grl_emotion): GradientReverseLayer()\n",
       "  (grl_facial_hair): GradientReverseLayer()\n",
       "  (grl_pose): GradientReverseLayer()\n",
       "  (grl_spectacles): GradientReverseLayer()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c40c23",
   "metadata": {},
   "source": [
    "emotion: sai task 0 toàn đoán thành task 2\n",
    "occlusion + pose: tăng khả năng phân biệt giữa các lớp\n",
    "spectales: tốt nhưng cần hơn\n",
    "facial_hair + gender: quá tốt không cần chỉnh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703da67",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(0), Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3162, 0.6838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.9972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8657, 0.1343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8622, 0.1378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1003/2008-02-21_16-38-47.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41c328",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(2), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30edffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2285, 0.7715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8189, 0.1811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0703, 0.9297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1183, 0.1410, 0.7407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0961, 0.5510, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5269, 0.0341, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/2186/2007-12-04_12-15-04.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86351519",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (1), Facial_Hair (0), Pose(0), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8e61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1420, 0.8580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8690, 0.1310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4120, 0.4890, 0.0990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2312, 0.3471, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6135, 0.0481, 0.3384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1009/2008-02-18_08-50-39.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc40bd",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (0), Pose(0), Occlusion (1),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05f7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1173, 0.8827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.9733, 0.0267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3992, 0.3661, 0.2347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1896, 0.4835, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4071, 0.0306, 0.5623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1038/2009-07-10_09-49-50.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f11c59",
   "metadata": {},
   "source": [
    "        # 0: nhìn trực diện (2471), 1: nhìn nghiêng 1 chút (326), 2: lệch 30-45 độ (77)\n",
    "        self.pose_loss = FocalLoss(alpha_weights={0: 0.0246, 1: 0.1863, 2: 0.7891}, gamma_weights={0: 1, 1: 0.5, 2: 0}, num_classes=3)\n",
    "        # 0: tóc che mặt (13), 1: tay che mặt (46), 2: không bị che khuất (2615)\n",
    "        self.occlusion_loss = FocalLoss(alpha_weights={0:0.7765, 1:0.2195, 2:0.0039}, gamma_weights={0: 0, 1: 0, 2: 1.5}, num_classes=3)\n",
    "        # 0: nhìn trực diện (2209), 1: các cảm xúc khác (249), 2: tích cực (416)\n",
    "        self.emotion_loss = FocalLoss(alpha_weights={0:0.0659, 1:0.5844, 2:0.3497}, gamma_weights={0: 0.5, 1: 0, 2: 0}, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980546",
   "metadata": {},
   "source": [
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 10,\n",
    "    'loss_da_gender_weight': 10,\n",
    "    'loss_emotion_weight': 10,\n",
    "    'loss_da_emotion_weight': 10,\n",
    "    'loss_pose_weight': 20,\n",
    "    'loss_da_pose_weight': 20,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_da_spectacles_weight': 5,\n",
    "    'loss_occlusion_weight': 20,\n",
    "    'loss_da_occlusion_weight': 20,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "    'loss_da_facial_hair_weight': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56044d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6338037,
     "sourceId": 10247612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17096.367763,
   "end_time": "2024-12-23T23:28:48.754749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T18:43:52.386986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
