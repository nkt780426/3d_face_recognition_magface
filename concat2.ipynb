{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c6f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T18:44:18.731566Z",
     "iopub.status.busy": "2024-12-23T18:44:18.731295Z",
     "iopub.status.idle": "2024-12-23T18:44:37.075895Z",
     "shell.execute_reply": "2024-12-23T18:44:37.074927Z"
    },
    "papermill": {
     "duration": 18.351133,
     "end_time": "2024-12-23T18:44:37.077621",
     "exception": false,
     "start_time": "2024-12-23T18:44:18.726488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from going_modular.dataloader.multitask import create_concatv2_multitask_datafetcher\n",
    "from going_modular.model.MTLFaceRecognition import MTLFaceRecognition\n",
    "from going_modular.model.ConcatMTLFaceRecognition import ConcatMTLFaceRecognitionV2\n",
    "from going_modular.loss.ConcatMultiTaskLoss import ConcatMultiTaskLoss\n",
    "from going_modular.train_eval.concat_train import fit\n",
    "from going_modular.utils.transforms import RandomResizedCropRect, GaussianNoise\n",
    "from going_modular.utils.MultiMetricEarlyStopping import MultiMetricEarlyStopping\n",
    "from going_modular.utils.ModelCheckPoint import ModelCheckpoint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Đặt seed toàn cục\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CONFIGURATION = {\n",
    "    'type': 'concat2',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'dataset_dir_1': './Dataset/Albedo',\n",
    "    'dataset_dir_2': './Dataset/Normal_Map',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    'checkpoint_1': './checkpoint/multi/albedo/models/checkpoint.pth',\n",
    "    'checkpoint_2': './checkpoint/multi/normalmap/models/checkpoint.pth',\n",
    "        \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 30,\n",
    "    'loss_emotion_weight': 5,\n",
    "    'loss_pose_weight': 30,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "}\n",
    "\n",
    "CONFIGURATION['num_classes'] = len(os.listdir('./Dataset/Albedo/train'))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    RandomResizedCropRect(256),\n",
    "    GaussianNoise(),\n",
    "], additional_targets={\n",
    "    'image2': 'image'\n",
    "})\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=CONFIGURATION['image_size'], width=CONFIGURATION['image_size'])\n",
    "], additional_targets={\n",
    "    'image2': 'image'\n",
    "})\n",
    "\n",
    "train_dataloader, test_dataloader = create_concatv2_multitask_datafetcher(CONFIGURATION, train_transform, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f392054",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = torch.load(CONFIGURATION['checkpoint_1'], map_location=torch.device('cpu'))\n",
    "mtl_backbone1 = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "mtl_backbone1.load_state_dict(checkpoint_1['model_state_dict'])\n",
    "\n",
    "checkpoint_2 = torch.load(CONFIGURATION['checkpoint_2'], map_location=torch.device('cpu'))\n",
    "mtl_backbone2 = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "mtl_backbone2.load_state_dict(checkpoint_2['model_state_dict'])\n",
    "\n",
    "model = ConcatMTLFaceRecognitionV2(mtl_backbone1, mtl_backbone2, CONFIGURATION['num_classes'])\n",
    "\n",
    "for param in model.mtl_backbone1.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.mtl_backbone2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6460a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ConcatMultiTaskLoss(os.path.join(CONFIGURATION['dataset_dir'], 'train_set.csv'), CONFIGURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e073a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=CONFIGURATION['base_lr'])\n",
    "# Khởi tạo scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "earlystop_dir = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models')\n",
    "\n",
    "early_stopping = MultiMetricEarlyStopping(\n",
    "    monitor_keys=['cosine_auc', 'euclidean_auc'],\n",
    "    patience=1000,\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_dir=earlystop_dir,\n",
    "    start_from_epoch=0\n",
    ")      \n",
    "checkpoint_path = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models/checkpoint.pth')\n",
    "modle_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(\n",
    "    conf=CONFIGURATION,\n",
    "    start_epoch=0,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,    \n",
    "    scheduler=scheduler, \n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=modle_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97f2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTLFaceRecognition(\n",
       "  (backbone): MIResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu): PReLU(num_parameters=1)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (spectacles_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (facial_hair_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (emotion_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (pose_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (gender_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (id_head): IdRecognitionModule(\n",
       "    (id_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (maglinear): MagLinear()\n",
       "  )\n",
       "  (gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (grl_gender): GradientReverseLayer()\n",
       "  (grl_emotion): GradientReverseLayer()\n",
       "  (grl_facial_hair): GradientReverseLayer()\n",
       "  (grl_pose): GradientReverseLayer()\n",
       "  (grl_spectacles): GradientReverseLayer()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c40c23",
   "metadata": {},
   "source": [
    "emotion: sai task 0 toàn đoán thành task 2\n",
    "occlusion + pose: tăng khả năng phân biệt giữa các lớp\n",
    "spectales: tốt nhưng cần hơn\n",
    "facial_hair + gender: quá tốt không cần chỉnh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703da67",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(0), Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3162, 0.6838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.9972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8657, 0.1343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8622, 0.1378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1003/2008-02-21_16-38-47.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_spectacles = model.get_result(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41c328",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(2), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30edffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2285, 0.7715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8189, 0.1811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0703, 0.9297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1183, 0.1410, 0.7407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0961, 0.5510, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5269, 0.0341, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/2186/2007-12-04_12-15-04.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_result(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86351519",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (1), Facial_Hair (0), Pose(0), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8e61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1420, 0.8580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8690, 0.1310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4120, 0.4890, 0.0990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2312, 0.3471, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6135, 0.0481, 0.3384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1009/2008-02-18_08-50-39.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_result(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc40bd",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (0), Pose(0), Occlusion (1),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05f7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1173, 0.8827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.9733, 0.0267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3992, 0.3661, 0.2347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1896, 0.4835, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4071, 0.0306, 0.5623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1038/2009-07-10_09-49-50.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_result(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f11c59",
   "metadata": {},
   "source": [
    "        # 0: nhìn trực diện (2471), 1: nhìn nghiêng 1 chút (326), 2: lệch 30-45 độ (77)\n",
    "        self.pose_loss = FocalLoss(alpha_weights={0: 0.0246, 1: 0.1863, 2: 0.7891}, gamma_weights={0: 1, 1: 0.5, 2: 0}, num_classes=3)\n",
    "        # 0: tóc che mặt (13), 1: tay che mặt (46), 2: không bị che khuất (2615)\n",
    "        self.occlusion_loss = FocalLoss(alpha_weights={0:0.7765, 1:0.2195, 2:0.0039}, gamma_weights={0: 0, 1: 0, 2: 1.5}, num_classes=3)\n",
    "        # 0: nhìn trực diện (2209), 1: các cảm xúc khác (249), 2: tích cực (416)\n",
    "        self.emotion_loss = FocalLoss(alpha_weights={0:0.0659, 1:0.5844, 2:0.3497}, gamma_weights={0: 0.5, 1: 0, 2: 0}, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980546",
   "metadata": {},
   "source": [
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 10,\n",
    "    'loss_da_gender_weight': 10,\n",
    "    'loss_emotion_weight': 10,\n",
    "    'loss_da_emotion_weight': 10,\n",
    "    'loss_pose_weight': 20,\n",
    "    'loss_da_pose_weight': 20,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_da_spectacles_weight': 5,\n",
    "    'loss_occlusion_weight': 20,\n",
    "    'loss_da_occlusion_weight': 20,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "    'loss_da_facial_hair_weight': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56044d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6338037,
     "sourceId": 10247612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "doan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17096.367763,
   "end_time": "2024-12-23T23:28:48.754749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T18:43:52.386986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
