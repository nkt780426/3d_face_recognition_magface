{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c6f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T18:44:18.731566Z",
     "iopub.status.busy": "2024-12-23T18:44:18.731295Z",
     "iopub.status.idle": "2024-12-23T18:44:37.075895Z",
     "shell.execute_reply": "2024-12-23T18:44:37.074927Z"
    },
    "papermill": {
     "duration": 18.351133,
     "end_time": "2024-12-23T18:44:37.077621",
     "exception": false,
     "start_time": "2024-12-23T18:44:18.726488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 15:08:15.472862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 15:08:16.636682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from going_modular.dataloader.multitask import create_multitask_datafetcher\n",
    "from going_modular.model.MTLFaceRecognition import MTLFaceRecognition\n",
    "from going_modular.loss.MultiTaskLoss import MultiTaskLoss\n",
    "from going_modular.train_eval.train import fit\n",
    "from going_modular.utils.transforms import RandomResizedCropRect, GaussianNoise\n",
    "from going_modular.utils.MultiMetricEarlyStopping import MultiMetricEarlyStopping\n",
    "from going_modular.utils.ModelCheckPoint import ModelCheckpoint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Đặt seed toàn cục\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 30,\n",
    "    'loss_da_gender_weight': 30,\n",
    "    'loss_emotion_weight': 10,\n",
    "    'loss_da_emotion_weight': 10,\n",
    "    'loss_pose_weight': 30,\n",
    "    'loss_da_pose_weight': 30,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_da_spectacles_weight': 5,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "    'loss_da_facial_hair_weight': 5,\n",
    "}\n",
    "\n",
    "CONFIGURATION['num_classes'] = len(os.listdir('./Dataset/Albedo/train'))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    RandomResizedCropRect(256),\n",
    "    GaussianNoise(),\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=CONFIGURATION['image_size'], width=CONFIGURATION['image_size'])\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "train_dataloader, test_dataloader, train_weight_class = create_multitask_datafetcher(CONFIGURATION, train_transform, test_transform)\n",
    "model = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "\n",
    "criterion = MultiTaskLoss(os.path.join(CONFIGURATION['dataset_dir'], 'train_set.csv'), CONFIGURATION)\n",
    "optimizer = Adam(model.parameters(), lr=CONFIGURATION['base_lr'])\n",
    "# Khởi tạo scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=40, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "earlystop_dir = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models')\n",
    "\n",
    "early_stopping = MultiMetricEarlyStopping(\n",
    "    monitor_keys=['cosine_auc', 'euclidean_auc'],\n",
    "    patience=1000,\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_dir=earlystop_dir,\n",
    "    start_from_epoch=0\n",
    ")      \n",
    "checkpoint_path = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models/checkpoint.pth')\n",
    "modle_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f42c339",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIGURATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodle_checkpoint\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/going_modular/train_eval/train.py:49\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(conf, start_epoch, model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, early_stopping, model_checkpoint)\u001b[0m\n\u001b[1;32m     32\u001b[0m device \u001b[38;5;241m=\u001b[39m conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     36\u001b[0m     (   \n\u001b[1;32m     37\u001b[0m         train_loss,\n\u001b[1;32m     38\u001b[0m         train_loss_id,\n\u001b[1;32m     39\u001b[0m         train_loss_gender,\n\u001b[1;32m     40\u001b[0m         train_loss_da_gender,\n\u001b[1;32m     41\u001b[0m         train_loss_emotion,\n\u001b[1;32m     42\u001b[0m         train_loss_da_emotion,\n\u001b[1;32m     43\u001b[0m         train_loss_pose,\n\u001b[1;32m     44\u001b[0m         train_loss_da_pose,\n\u001b[1;32m     45\u001b[0m         train_loss_facial_hair,\n\u001b[1;32m     46\u001b[0m         train_loss_da_facial_hair,\n\u001b[1;32m     47\u001b[0m         train_loss_spectacles,\n\u001b[1;32m     48\u001b[0m         train_loss_da_spectacles\n\u001b[0;32m---> 49\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     (   \n\u001b[1;32m     52\u001b[0m         test_loss_gender,\n\u001b[1;32m     53\u001b[0m         test_loss_da_gender,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         test_loss_da_spectacles\n\u001b[1;32m     62\u001b[0m     ) \u001b[38;5;241m=\u001b[39m test_epoch(test_dataloader, model, criterion, device)\n\u001b[1;32m     64\u001b[0m     train_auc \u001b[38;5;241m=\u001b[39m compute_auc(train_dataloader, model, device)\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/going_modular/train_eval/train.py:229\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    225\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    227\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 229\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m (\n\u001b[1;32m    232\u001b[0m     total_loss, \n\u001b[1;32m    233\u001b[0m     loss_id, loss_gender, loss_da_gender, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     loss_spectacles, loss_da_spectacles\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m    240\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/going_modular/model/MTLFaceRecognition.py:61\u001b[0m, in \u001b[0;36mMTLFaceRecognition.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m x_spectacles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectacles_head(x_spectacles)\n\u001b[1;32m     59\u001b[0m x_da_spectacles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mda_spectacles_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrl_spectacles(x_non_spectacles))\n\u001b[0;32m---> 61\u001b[0m x_facial_hair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfacial_hair_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_facial_hair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m x_da_facial_hair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mda_facial_hair_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrl_facial_hair(x_non_facial_hair))\n\u001b[1;32m     64\u001b[0m x_pose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_head(x_pose)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition_magface/going_modular/model/head/facialhair.py:21\u001b[0m, in \u001b[0;36mFacialHairDetectModule.forward\u001b[0;34m(self, x_facial_hair)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_facial_hair):\n\u001b[1;32m     20\u001b[0m     x_facial_hair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfacial_hair_embedding(x_facial_hair)\n\u001b[0;32m---> 21\u001b[0m     x_facial_hair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfacial_hair_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_facial_hair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_facial_hair\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "fit(\n",
    "    conf=CONFIGURATION,\n",
    "    start_epoch=0,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,    \n",
    "    scheduler=scheduler, \n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=modle_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97f2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTLFaceRecognition(\n",
       "  (backbone): MIResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu): PReLU(num_parameters=1)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (spectacles_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (facial_hair_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (emotion_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (pose_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (gender_fsm): AttentionModule(\n",
       "      (avg_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (max_spp): SPPModule(\n",
       "        (pool_blocks): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=2)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): AdaptiveMaxPool2d(output_size=3)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial): Sequential(\n",
       "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (channel): Sequential(\n",
       "        (0): Conv2d(7168, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (id_head): IdRecognitionModule(\n",
       "    (id_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (maglinear): MagLinear()\n",
       "  )\n",
       "  (gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_gender_head): GenderDetectModule(\n",
       "    (gender_output_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_emotion_head): EmotionDetectModule(\n",
       "    (emotion_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_facial_hair_head): FacialHairDetectModule(\n",
       "    (facial_hair_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_pose_head): PoseDetectModule(\n",
       "    (pose_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (da_spectacles_head): SpectacleDetectModule(\n",
       "    (spectacle_ouput_layer): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (2): Flatten(start_dim=1, end_dim=-1)\n",
       "      (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (grl_gender): GradientReverseLayer()\n",
       "  (grl_emotion): GradientReverseLayer()\n",
       "  (grl_facial_hair): GradientReverseLayer()\n",
       "  (grl_pose): GradientReverseLayer()\n",
       "  (grl_spectacles): GradientReverseLayer()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c40c23",
   "metadata": {},
   "source": [
    "emotion: sai task 0 toàn đoán thành task 2\n",
    "occlusion + pose: tăng khả năng phân biệt giữa các lớp\n",
    "spectales: tốt nhưng cần hơn\n",
    "facial_hair + gender: quá tốt không cần chỉnh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703da67",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(0), Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3162, 0.6838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0028, 0.9972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8657, 0.1343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8622, 0.1378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1003/2008-02-21_16-38-47.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41c328",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (1), Pose(2), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30edffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2285, 0.7715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8189, 0.1811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0703, 0.9297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1183, 0.1410, 0.7407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0961, 0.5510, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5269, 0.0341, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/2186/2007-12-04_12-15-04.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86351519",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (1), Facial_Hair (0), Pose(0), Occlusion (2),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8e61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1420, 0.8580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0030, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.8690, 0.1310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4120, 0.4890, 0.0990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2312, 0.3471, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6135, 0.0481, 0.3384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1009/2008-02-18_08-50-39.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc40bd",
   "metadata": {},
   "source": [
    "Gender (1), Spectacles (0), Facial_Hair (0), Pose(0), Occlusion (1),Emotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05f7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1173, 0.8827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.9733, 0.0267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3992, 0.3661, 0.2347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1896, 0.4835, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4071, 0.0306, 0.5623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = './Dataset/Albedo/gallery/1038/2009-07-10_09-49-50.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)\n",
    "x_gender = torch.softmax(x_gender, dim=1)\n",
    "print(x_gender)\n",
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "print(x_spectacles)\n",
    "x_facial_hair = torch.softmax(x_facial_hair, dim=1)\n",
    "print(x_facial_hair)\n",
    "x_pose = torch.softmax(x_pose, dim=1)\n",
    "print(x_pose)\n",
    "x_occlusion = torch.softmax(x_occlusion, dim=1)\n",
    "print(x_occlusion)\n",
    "x_emotion = torch.softmax(x_emotion, dim=1)\n",
    "print(x_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f11c59",
   "metadata": {},
   "source": [
    "        # 0: nhìn trực diện (2471), 1: nhìn nghiêng 1 chút (326), 2: lệch 30-45 độ (77)\n",
    "        self.pose_loss = FocalLoss(alpha_weights={0: 0.0246, 1: 0.1863, 2: 0.7891}, gamma_weights={0: 1, 1: 0.5, 2: 0}, num_classes=3)\n",
    "        # 0: tóc che mặt (13), 1: tay che mặt (46), 2: không bị che khuất (2615)\n",
    "        self.occlusion_loss = FocalLoss(alpha_weights={0:0.7765, 1:0.2195, 2:0.0039}, gamma_weights={0: 0, 1: 0, 2: 1.5}, num_classes=3)\n",
    "        # 0: nhìn trực diện (2209), 1: các cảm xúc khác (249), 2: tích cực (416)\n",
    "        self.emotion_loss = FocalLoss(alpha_weights={0:0.0659, 1:0.5844, 2:0.3497}, gamma_weights={0: 0.5, 1: 0, 2: 0}, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980546",
   "metadata": {},
   "source": [
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 39,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 1e-4,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_gender_weight': 10,\n",
    "    'loss_da_gender_weight': 10,\n",
    "    'loss_emotion_weight': 10,\n",
    "    'loss_da_emotion_weight': 10,\n",
    "    'loss_pose_weight': 20,\n",
    "    'loss_da_pose_weight': 20,\n",
    "    'loss_spectacles_weight': 5,\n",
    "    'loss_da_spectacles_weight': 5,\n",
    "    'loss_occlusion_weight': 20,\n",
    "    'loss_da_occlusion_weight': 20,\n",
    "    'loss_facial_hair_weight': 5,\n",
    "    'loss_da_facial_hair_weight': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56044d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6338037,
     "sourceId": 10247612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17096.367763,
   "end_time": "2024-12-23T23:28:48.754749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T18:43:52.386986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
